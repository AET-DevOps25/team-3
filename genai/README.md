# ğŸ§  GenAI & RAG Pipeline

This module implements the core GenAI capabilities for **Study Mate**, leveraging Retrieval-Augmented Generation (RAG) and LangChain to enable document-aware responses, summaries, flashcards, and quizzes.


## ğŸ” What It Does

When a user uploads a `.pdf` or `.txt` document, the system:

- âœ… Parses and splits the content into meaningful chunks
- ğŸ§  Embeds content using HuggingFace (MiniLM) and stores in **Weaviate**
- ğŸ’¬ Supports document-specific chat using RAG
- âœï¸ Generates:
  - Structured **summaries** (Markdown)
  - **Flashcards** (difficulty-tagged)
  - **Quizzes** (MCQ and short-answer)


## ğŸ“ Key Files

| File | Description |
|------|-------------|
| `llm.py` | Manages all GenAI functionality: chat, summarization, flashcards, quiz generation |
| `rag.py` | Handles ingestion, chunking, metadata, vector embedding, and retrieval via Weaviate |
| `chains.py` | Defines custom LangChain chains to generate structured outputs (flashcards, quizzes) |


## ğŸ§  Architecture Overview

- ğŸ”— **LangChain** for LLM orchestration
- ğŸ“š **Weaviate** stores two types of chunks:
  - `RAGChunksIndex`: Vectorized, small chunks for semantic search
  - `GenerationChunksIndex`: Larger, plain text chunks for generative tasks (e.g., summaries)
- ğŸ¤— **HuggingFace** MiniLM used for embeddings
- ğŸŒ **Open WebUI-compatible API** (LLaMA 3) for LLM calls


## ğŸ—‚ Workflow

1. **Load document**  
   â†’ via `StudyLLM.load_document(doc_name, path, user_id)`

2. **Ingest chunks**  
   â†’ Embedded RAG chunks go to `RAGChunksIndex`  
   â†’ Generation chunks go to `GenerationChunksIndex`

3. **Chat**  
   â†’ Queries are filtered by `user_id` and optionally `doc_name`  
   â†’ Top-k relevant chunks retrieved from Weaviate and passed to the LLM

4. **Summarize / Flashcards / Quiz**  
   â†’ Uses larger plain-text chunks stored per document  
   â†’ LangChainâ€™s **map-reduce** pattern **parallelized** to generate structured output


## ğŸ§ª Available Features

| Feature | Method |
|--------|--------|
| Load Document | `load_document(doc_name, path, user_id)` |
| RAG Chat | `prompt(prompt, user_id)` |
| Summarize | `summarize(document_name, user_id)` |
| Flashcards | `generate_flashcards(document_name, user_id)` |
| Quiz | `generate_quiz(document_name, user_id)` |
| Cleanup  (see below) | `cleanup()` |

## ğŸ“Œ Notes

- Only `.pdf` and `.txt` documents are supported.

## ğŸ“¦ Dependencies

- `langchain`
- `langchain-openai`
- `langchain-huggingface`
- `langchain-community`
- `weaviate-client`
- `PyMuPDF`, `dotenv`, `asyncio`, etc.


## ğŸ§¹ Cleanup

Call `StudyLLM.cleanup()` to close the Weaviate client connection properly.


## ğŸ“„ License

MIT â€” see [LICENSE](../LICENSE)
